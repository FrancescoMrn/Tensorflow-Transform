{
 "cells": [
  {
   "cell_type": "markdown",
   "id": "placed-chocolate",
   "metadata": {},
   "source": [
    "# House Price - Data Processing with TF.Transform\n",
    "\n",
    "This notebook will report all the required code to create and export a data processing pipeline using Apache Beam and TF.Transform.\n",
    "\n",
    "Whenever possible the notebook will be coded to support cloud run on GCP and local run."
   ]
  },
  {
   "cell_type": "markdown",
   "id": "dense-liability",
   "metadata": {},
   "source": [
    "## Set global parameters"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 1,
   "id": "imperial-crazy",
   "metadata": {},
   "outputs": [],
   "source": [
    "PROJECT = 'gcp-playground' # change to your project_Id\n",
    "BUCKET = 'gcs-cloudml'     # change to your bucket name\n",
    "REGION = 'region'          # change to your region\n",
    "\n",
    "\n",
    "ROOT_DIR = 'data'          # directory where the data are located is stored locally or on GCS\n",
    "INPUT_FILE = \"train.csv\"\n",
    "OUTPUT_DIR = \"data/output\""
   ]
  },
  {
   "cell_type": "markdown",
   "id": "accessory-usage",
   "metadata": {},
   "source": [
    "## Import required modules"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 2,
   "id": "increased-circle",
   "metadata": {},
   "outputs": [],
   "source": [
    "import os\n",
    "os.environ['PROJECT'] = PROJECT\n",
    "os.environ['BUCKET'] = BUCKET\n",
    "os.environ['REGION'] = REGION\n",
    "os.environ['ROOT_DIR'] = ROOT_DIR\n",
    "os.environ['RUN_LOCAL'] = str(RUN_LOCAL)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 3,
   "id": "declared-directory",
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Tensorflow version: 2.3.2\n"
     ]
    }
   ],
   "source": [
    "# Import data processing libraries\n",
    "import pandas as pd\n",
    "import tensorflow as tf\n",
    "# Import Tensorflow Transform\n",
    "import tensorflow_transform as tft\n",
    "from tensorflow_transform.tf_metadata import dataset_metadata\n",
    "from tensorflow_transform.tf_metadata import dataset_schema\n",
    "# Import Apache Beam\n",
    "import apache_beam as beam\n",
    "from apache_beam.options.pipeline_options import PipelineOptions\n",
    "# Python shutil module enables us to operate with file objects easily and without diving into file objects a lot.\n",
    "import shutil\n",
    "# Show the currently installed version of TensorFlow\n",
    "print(\"Tensorflow version: {}\".format(tf.__version__))"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "built-marathon",
   "metadata": {},
   "source": [
    "## Input source: CSV Local"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 4,
   "id": "recent-marathon",
   "metadata": {},
   "outputs": [],
   "source": [
    "# Import data locally only the fist 100 rows to support\n",
    "# the creation of the processing functions\n",
    "\n",
    "# TODO update path to bucket\n",
    "path_to_csv = os.path.join(ROOT_DIR, INPUT_FILE)\n",
    "data = pd.read_csv(path_to_csv, nrows=100)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 13,
   "id": "synthetic-aging",
   "metadata": {},
   "outputs": [
    {
     "data": {
      "text/plain": [
       "Index(['POSTED_BY', 'UNDER_CONSTRUCTION', 'RERA', 'BHK_NO.', 'BHK_OR_RK',\n",
       "       'SQUARE_FT', 'READY_TO_MOVE', 'RESALE', 'ADDRESS', 'LONGITUDE',\n",
       "       'LATITUDE', 'TARGET(PRICE_IN_LACS)'],\n",
       "      dtype='object')"
      ]
     },
     "execution_count": 13,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "data.columns"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 5,
   "id": "accessory-validity",
   "metadata": {},
   "outputs": [
    {
     "data": {
      "text/html": [
       "<div>\n",
       "<style scoped>\n",
       "    .dataframe tbody tr th:only-of-type {\n",
       "        vertical-align: middle;\n",
       "    }\n",
       "\n",
       "    .dataframe tbody tr th {\n",
       "        vertical-align: top;\n",
       "    }\n",
       "\n",
       "    .dataframe thead th {\n",
       "        text-align: right;\n",
       "    }\n",
       "</style>\n",
       "<table border=\"1\" class=\"dataframe\">\n",
       "  <thead>\n",
       "    <tr style=\"text-align: right;\">\n",
       "      <th></th>\n",
       "      <th>POSTED_BY</th>\n",
       "      <th>UNDER_CONSTRUCTION</th>\n",
       "      <th>RERA</th>\n",
       "      <th>BHK_NO.</th>\n",
       "      <th>BHK_OR_RK</th>\n",
       "      <th>SQUARE_FT</th>\n",
       "      <th>READY_TO_MOVE</th>\n",
       "      <th>RESALE</th>\n",
       "      <th>ADDRESS</th>\n",
       "      <th>LONGITUDE</th>\n",
       "      <th>LATITUDE</th>\n",
       "      <th>TARGET(PRICE_IN_LACS)</th>\n",
       "    </tr>\n",
       "  </thead>\n",
       "  <tbody>\n",
       "    <tr>\n",
       "      <th>0</th>\n",
       "      <td>Owner</td>\n",
       "      <td>0</td>\n",
       "      <td>0</td>\n",
       "      <td>2</td>\n",
       "      <td>BHK</td>\n",
       "      <td>1300.236407</td>\n",
       "      <td>1</td>\n",
       "      <td>1</td>\n",
       "      <td>Ksfc Layout,Bangalore</td>\n",
       "      <td>12.969910</td>\n",
       "      <td>77.597960</td>\n",
       "      <td>55.0</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>1</th>\n",
       "      <td>Dealer</td>\n",
       "      <td>0</td>\n",
       "      <td>0</td>\n",
       "      <td>2</td>\n",
       "      <td>BHK</td>\n",
       "      <td>1275.000000</td>\n",
       "      <td>1</td>\n",
       "      <td>1</td>\n",
       "      <td>Vishweshwara Nagar,Mysore</td>\n",
       "      <td>12.274538</td>\n",
       "      <td>76.644605</td>\n",
       "      <td>51.0</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>2</th>\n",
       "      <td>Owner</td>\n",
       "      <td>0</td>\n",
       "      <td>0</td>\n",
       "      <td>2</td>\n",
       "      <td>BHK</td>\n",
       "      <td>933.159722</td>\n",
       "      <td>1</td>\n",
       "      <td>1</td>\n",
       "      <td>Jigani,Bangalore</td>\n",
       "      <td>12.778033</td>\n",
       "      <td>77.632191</td>\n",
       "      <td>43.0</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>3</th>\n",
       "      <td>Owner</td>\n",
       "      <td>0</td>\n",
       "      <td>1</td>\n",
       "      <td>2</td>\n",
       "      <td>BHK</td>\n",
       "      <td>929.921143</td>\n",
       "      <td>1</td>\n",
       "      <td>1</td>\n",
       "      <td>Sector-1 Vaishali,Ghaziabad</td>\n",
       "      <td>28.642300</td>\n",
       "      <td>77.344500</td>\n",
       "      <td>62.5</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>4</th>\n",
       "      <td>Dealer</td>\n",
       "      <td>1</td>\n",
       "      <td>0</td>\n",
       "      <td>2</td>\n",
       "      <td>BHK</td>\n",
       "      <td>999.009247</td>\n",
       "      <td>0</td>\n",
       "      <td>1</td>\n",
       "      <td>New Town,Kolkata</td>\n",
       "      <td>22.592200</td>\n",
       "      <td>88.484911</td>\n",
       "      <td>60.5</td>\n",
       "    </tr>\n",
       "  </tbody>\n",
       "</table>\n",
       "</div>"
      ],
      "text/plain": [
       "  POSTED_BY  UNDER_CONSTRUCTION  RERA  BHK_NO. BHK_OR_RK    SQUARE_FT  \\\n",
       "0     Owner                   0     0        2       BHK  1300.236407   \n",
       "1    Dealer                   0     0        2       BHK  1275.000000   \n",
       "2     Owner                   0     0        2       BHK   933.159722   \n",
       "3     Owner                   0     1        2       BHK   929.921143   \n",
       "4    Dealer                   1     0        2       BHK   999.009247   \n",
       "\n",
       "   READY_TO_MOVE  RESALE                      ADDRESS  LONGITUDE   LATITUDE  \\\n",
       "0              1       1        Ksfc Layout,Bangalore  12.969910  77.597960   \n",
       "1              1       1    Vishweshwara Nagar,Mysore  12.274538  76.644605   \n",
       "2              1       1             Jigani,Bangalore  12.778033  77.632191   \n",
       "3              1       1  Sector-1 Vaishali,Ghaziabad  28.642300  77.344500   \n",
       "4              0       1             New Town,Kolkata  22.592200  88.484911   \n",
       "\n",
       "   TARGET(PRICE_IN_LACS)  \n",
       "0                   55.0  \n",
       "1                   51.0  \n",
       "2                   43.0  \n",
       "3                   62.5  \n",
       "4                   60.5  "
      ]
     },
     "execution_count": 5,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "data.head()"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 6,
   "id": "generic-wrist",
   "metadata": {},
   "outputs": [],
   "source": [
    "# Convert csv in a bigquery like processor\n",
    "data_bq_like = list(data.iloc[:5].to_dict(\"index\").values()) # sample just the first 10 rows to test the processing functions"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "obvious-arbitration",
   "metadata": {},
   "source": [
    "## Create raw data metadata"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 7,
   "id": "approximate-burner",
   "metadata": {},
   "outputs": [],
   "source": [
    "CATEGORICAL_FEATURE_NAMES = ['POSTED_BY', 'UNDER_CONSTRUCTION', 'RERA', \n",
    "                             'BHK_OR_RK', 'READY_TO_MOVE', 'RESALE', \n",
    "                             'ADDRESS']\n",
    "#CATEGORICAL_FEATURE_MAX_VALUES = [24]\n",
    "\n",
    "NUMERIC_FEATURE_NAMES = ['BHK_NO.',  'SQUARE_FT', 'LONGITUDE','LATITUDE']\n",
    "#BUCKET_FEATURE_BUCKET_COUNT = [100]\n",
    "\n",
    "TARGET_FEATURE_NAME = \"TARGET(PRICE_IN_LACS)\""
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 8,
   "id": "presidential-whale",
   "metadata": {},
   "outputs": [],
   "source": [
    "def create_raw_metadata():\n",
    "    \n",
    "    raw_data_schema = {}\n",
    "    \n",
    " \n",
    "    # target feature schema\n",
    "    raw_data_schema[TARGET_FEATURE_NAME]= dataset_schema.ColumnSchema(\n",
    "            tf.float32, [], dataset_schema.FixedColumnRepresentation())\n",
    "    \n",
    "    # categorical features schema\n",
    "    raw_data_schema.update({column_name : dataset_schema.ColumnSchema(\n",
    "        tf.string, [], dataset_schema.FixedColumnRepresentation())\n",
    "                            for column_name in CATEGORICAL_FEATURE_NAMES})\n",
    "    \n",
    "    # numerical features schema\n",
    "    raw_data_schema.update({ column_name : dataset_schema.ColumnSchema(\n",
    "        tf.float32, [], dataset_schema.FixedColumnRepresentation())\n",
    "                            for column_name in NUMERIC_FEATURE_NAMES})\n",
    "    \n",
    "    # create dataset_metadata given raw_schema\n",
    "    raw_data_metadata = dataset_metadata.DatasetMetadata(\n",
    "        dataset_schema.Schema(raw_data_schema))\n",
    "    \n",
    "    return raw_data_metadata"
   ]
  },
  {
   "cell_type": "raw",
   "id": "imported-assurance",
   "metadata": {},
   "source": [
    "# DEBUG validation of data_schema function\n",
    "create_raw_metadata()"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "liberal-alliance",
   "metadata": {},
   "source": [
    "## Input source: BigQuery"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "standard-monaco",
   "metadata": {},
   "outputs": [],
   "source": [
    "from google.cloud import bigquery\n",
    "\n",
    "def create_query(phase, hash_column, table_name, EVERY_N):\n",
    "    \"\"\"Creates a query with the proper splits.\n",
    "    Args:\n",
    "        phase: str, train, valid.\n",
    "        hash_column: str, is the column that we will use to perform the split\n",
    "        EVERY_N: int, take an example EVERY_N rows.\n",
    "\n",
    "    Returns:\n",
    "        Query string with the proper splits.\n",
    "    \"\"\"\n",
    "    base_query = \"\"\"\n",
    "    SELECT * FROM {0}\n",
    "    \"\"\".format(table_name)\n",
    "    \n",
    "    if EVERY_N is None:\n",
    "        if phase==\"train\":\n",
    "            # training\n",
    "            query = \"\"\"{0} AND ABS(MOD(FARM_FINGERPRINT(CAST\n",
    "            ({1} AS STRING), 10)) < 8\"\"\".format(base_query, hash_column)\n",
    "        else:\n",
    "            query = \"\"\"{0} AND ABS(MOD(FARM_FINGERPRINT(CAST(\n",
    "            {1} AS STRING), 10)) > 8\"\"\".format(base_query, hash_column)\n",
    "    else:\n",
    "        query = \"\"\"{0} AND ABS(MOD(FARM_FINGERPRINT(CAST(\n",
    "        {1} AS STRING)), {2})) = 2\"\"\".format(base_query,hash_column, EVERY_N)\n",
    "\n",
    "    return query"
   ]
  },
  {
   "cell_type": "raw",
   "id": "rational-neighbor",
   "metadata": {},
   "source": [
    "# DEBUG validation of the BigQuery function\n",
    "query = create_query(\"valid\", \"ADDRESS\", <table_name>)\n",
    "df_validation = bigquery.Client().query(query).to_dataframe()\n",
    "# `head()` function is used to get the first n rows of dataframe\n",
    "display(df_validation.head())"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 10,
   "id": "immediate-liechtenstein",
   "metadata": {},
   "outputs": [],
   "source": [
    "def is_valid(inputs):\n",
    "    \"\"\"Check to make sure the inputs are valid.\n",
    "    Args:\n",
    "        inputs: dict, dictionary of TableRow data from BigQuery.\n",
    "\n",
    "    Returns:\n",
    "        True if the inputs are valid and False if they are not.\n",
    "    \"\"\"\n",
    "    try:\n",
    "        underconstruction = inputs[\"UNDER_CONSTRUCTION\"]\n",
    "        return underconstruction >= 0 and underconstruction  <2\n",
    "    except:\n",
    "        return False"
   ]
  },
  {
   "cell_type": "raw",
   "id": "hungarian-advice",
   "metadata": {},
   "source": [
    "# DEBUG validation of the is_valid function:\n",
    "for element in data_bq_like:\n",
    "    print(is_valid(element))"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 27,
   "id": "established-curve",
   "metadata": {},
   "outputs": [],
   "source": [
    "def preprocess_tft(inputs):\n",
    "    \"\"\"\n",
    "    Preprocess the features and add engineered features with tf transform.\n",
    "\n",
    "    Args:\n",
    "        dict, dictionary of TableRow data from BigQuery.\n",
    "\n",
    "    Returns:\n",
    "        Dictionary of preprocessed data after scaling and feature engineering.\n",
    "    \"\"\"\n",
    "    import datetime\n",
    "    print(inputs)\n",
    "    # create a new dictionary that will contain only those relevan features selected\n",
    "    result = {}\n",
    "    \n",
    "    # Report the label in the processed data set\n",
    "    result[\"target_price_lacs\"] = tf.identity(inputs[\"TARGET(PRICE_IN_LACS)\"])\n",
    "    \n",
    "    # Build a vocobulary\n",
    "    result[\"address\"] = tft.compute_and_apply_vocabulary(inputs[\"ADDRESS\"].split(\",\")[-1]) #remove the address and leave the city\n",
    "    result[\"posted_by\"] = tf.compute_and_apply_vocabulary(inputs[\"POSTED_BY\"])\n",
    "    result[\"bhk_or_rk\"] = tf.compute_and_apply_vocabulary(inputs[\"BHK_OR_RK\"])\n",
    "    \n",
    "    # Keep the  categorical features untouch\n",
    "    result[\"under_construction\"] = tf.identity(inputs[\"UNDER_CONSTRUCTION\"])\n",
    "    result[\"rera\"] = tf.identity(inputs[\"RERA\"])\n",
    "    result[\"ready_to_move\"] = tf.identity(inputs[\"READY_TO_MOVE\"])\n",
    "    result[\"resale\"] = tf.identity(inputs[\"RESALE\"])\n",
    "\n",
    "    # Scaling numeric values\n",
    "    result['BHK_NO'] = (tft.scale_to_0_1(inputs['BHK_NO']))\n",
    "    result['SQUARE_FT'] = (tft.scale_to_0_1(inputs['SQUARE_FT']))\n",
    "\n",
    "    # Bucketize\n",
    "    result['longitude_bucket'] = tft.bucketize(inputs[\"LONGITUDE\"], num_buckets=100)\n",
    "    result['latitude_bucket'] = tft.bucketize(inputs[\"LATITUDE\"], num_buckets=100)\n",
    "    \n",
    "    \n",
    "    # Engineered features\n",
    "    # basic implementation of a feature created to (potentially) improve the prediction\n",
    "    result[\"avg_room_sq.ft\"] = tft.scale_to_0_1(inputs['SQUARE_FT'] / inputs['BHK_NO'])\n",
    "    \n",
    "    return result"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "finnish-basic",
   "metadata": {},
   "source": [
    "## Process function Apache Beam"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "administrative-management",
   "metadata": {},
   "outputs": [],
   "source": [
    "def preprocess(in_test_mode):\n",
    "    \"\"\"Sets up preprocess pipeline.\n",
    "\n",
    "    Args:\n",
    "        in_test_mode: bool, False to launch DataFlow job, True to run locally.\n",
    "    \"\"\"\n",
    "    import os\n",
    "    import os.path\n",
    "    import tempfile\n",
    "    from apache_beam.io import tfrecordio\n",
    "    from tensorflow_transform.coders import example_proto_coder\n",
    "    from tensorflow_transform.tf_metadata import dataset_metadata\n",
    "    from tensorflow_transform.tf_metadata import dataset_schema\n",
    "    from tensorflow_transform.beam import tft_beam_io\n",
    "    from tensorflow_transform.beam.tft_beam_io import transform_fn_io\n",
    "\n",
    "    job_name = 'house_price_tftprocessing' + '-'\n",
    "    job_name += datetime.datetime.now().strftime('%y%m%d-%H%M%S')\n",
    "    if in_test_mode:\n",
    "        import shutil\n",
    "        print('Launching local job ...')\n",
    "        OUTPUT_DIR = './procesing_tft'\n",
    "        shutil.rmtree(OUTPUT_DIR, ignore_errors=True)\n",
    "        EVERY_N = 100\n",
    "    else:\n",
    "        print('Launching Dataflow job {} ...'.format(job_name))\n",
    "        OUTPUT_DIR = 'gs://{0}/houseprice/processing_tft/'.format(BUCKET)\n",
    "        import subprocess\n",
    "        subprocess.call('gsutil rm -r {}'.format(OUTPUT_DIR).split())\n",
    "        EVERY_N = None\n",
    "\n",
    "    options = {\n",
    "        'staging_location': os.path.join(OUTPUT_DIR, 'tmp', 'staging'),\n",
    "        'temp_location': os.path.join(OUTPUT_DIR, 'tmp'),\n",
    "        'job_name': job_name,\n",
    "        'project': PROJECT,\n",
    "        'num_workers': 1,\n",
    "        'max_num_workers': 1,\n",
    "        'teardown_policy': 'TEARDOWN_ALWAYS',\n",
    "        'no_save_main_session': True,\n",
    "        'direct_num_workers': 1,\n",
    "        'extra_packages': ['tensorflow_transform-0.24.0-py3-none-any.whl']\n",
    "        }\n",
    "\n",
    "    opts = beam.pipeline.PipelineOptions(flags=[], **options)\n",
    "    if in_test_mode:\n",
    "        RUNNER = 'DirectRunner'\n",
    "    else:\n",
    "        RUNNER = 'DataflowRunner'\n",
    "\n",
    "    # Set up raw data metadata\n",
    "    raw_data_schema = create_raw_metadata()\n",
    "\n",
    "    # Run Beam\n",
    "    with beam.Pipeline(RUNNER, options=opts) as p:\n",
    "        with beam_impl.Context(temp_dir=os.path.join(OUTPUT_DIR, 'tmp')):\n",
    "            # Save the raw data metadata\n",
    "            (raw_data_metadata |\n",
    "                'WriteInputMetadata' >> tft_beam_io.WriteMetadata(\n",
    "                    os.path.join(\n",
    "                        OUTPUT_DIR, 'metadata/rawdata_metadata'), pipeline=p))\n",
    "\n",
    "            # Read training data from bigquery and filter rows\n",
    "            raw_data = (p \n",
    "                        | 'Read train set' >> beam.io.Read(beam.io.BigQuerySource(query=create_query(\"train\", \"ADDRESS\", <table_name>),\n",
    "                                                                              use_standard_sql=True)) \n",
    "                        | 'Validate data' >> beam.Filter(is_valid))\n",
    "\n",
    "            raw_dataset = (raw_data, raw_data_metadata)\n",
    "\n",
    "            # Analyze and transform training data\n",
    "            transformed_dataset, transform_fn = (raw_dataset \n",
    "                                                 | beam_impl.AnalyzeAndTransformDataset(preprocess_tft))\n",
    "            \n",
    "            transformed_data, transformed_metadata = transformed_dataset\n",
    "\n",
    "            # Save transformed train data to disk in efficient tfrecord format\n",
    "            transformed_data | 'WriteTrainData' >> tfrecordio.WriteToTFRecord(os.path.join(OUTPUT_DIR, 'train'),\n",
    "                                                                              file_name_suffix='.gz',\n",
    "                                                                              coder=example_proto_coder.ExampleProtoCoder(transformed_metadata.schema))\n",
    "\n",
    "            \n",
    "            # Read eval data from bigquery and filter rows\n",
    "            raw_test_data = (p \n",
    "                             | 'Read eval set' >> beam.io.Read(beam.io.BigQuerySource(query=create_query(\"valid\", \"ADDRESS\", <table_name>),\n",
    "                                                                                        use_standard_sql=True)) \n",
    "                             | 'eval_filter' >> beam.Filter(is_valid))\n",
    "\n",
    "            raw_test_dataset = (raw_test_data, raw_data_metadata)\n",
    "\n",
    "            # Transform eval data\n",
    "            transformed_test_dataset = ((raw_test_dataset, transform_fn) | beam_impl.TransformDataset())\n",
    "            transformed_test_data, _ = transformed_test_dataset\n",
    "\n",
    "            # Save transformed train data to disk in efficient tfrecord format\n",
    "            (transformed_test_data | 'WriteTestData' >> tfrecordio.WriteToTFRecord(os.path.join(OUTPUT_DIR, 'eval'), \n",
    "                                                                                   file_name_suffix='.gz',\n",
    "                                                                                   coder=example_proto_coder.ExampleProtoCoder(transformed_metadata.schema)))\n",
    "\n",
    "            # Save transformation function to disk for use at serving time\n",
    "            (transform_fn | 'WriteTransformFn' >> transform_fn_io.WriteTransformFn(os.path.join(OUTPUT_DIR, 'metadata')))"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "canadian-timer",
   "metadata": {},
   "outputs": [],
   "source": [
    "# Change to False to run on DataFlow\n",
    "preprocess(in_test_mode=True)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "obvious-effects",
   "metadata": {},
   "outputs": [],
   "source": []
  }
 ],
 "metadata": {
  "kernelspec": {
   "display_name": "Python 3",
   "language": "python",
   "name": "python3"
  },
  "language_info": {
   "codemirror_mode": {
    "name": "ipython",
    "version": 3
   },
   "file_extension": ".py",
   "mimetype": "text/x-python",
   "name": "python",
   "nbconvert_exporter": "python",
   "pygments_lexer": "ipython3",
   "version": "3.7.9"
  }
 },
 "nbformat": 4,
 "nbformat_minor": 5
}
