{
 "cells": [
  {
   "cell_type": "markdown",
   "id": "velvet-currency",
   "metadata": {},
   "source": [
    "# House Price - Data Processing with TF.Transform\n",
    "\n",
    "This notebook will report all the required code to create and export a data processing pipeline using Apache Beam and TF.Transform.\n",
    "\n",
    "Whenever possible the notebook will be coded to support cloud run on GCP and local run."
   ]
  },
  {
   "cell_type": "markdown",
   "id": "german-stretch",
   "metadata": {},
   "source": [
    "## Set global parameters"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 1,
   "id": "creative-spring",
   "metadata": {},
   "outputs": [],
   "source": [
    "PROJECT = 'project'         # change to your project_Id\n",
    "BUCKET = 'houseprice_tftransform'     # change to your bucket name\n",
    "REGION = 'region'                     # change to your region\n",
    "\n",
    "\n",
    "LOCAL_ROOT_DIR = 'data'          # directory where the data are located is stored locally to support exploration\n",
    "INPUT_FILE = \"train.csv\""
   ]
  },
  {
   "cell_type": "markdown",
   "id": "rotary-aaron",
   "metadata": {},
   "source": [
    "## Import required modules"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 2,
   "id": "monthly-entry",
   "metadata": {},
   "outputs": [],
   "source": [
    "import os\n",
    "os.environ['PROJECT'] = PROJECT\n",
    "os.environ['BUCKET'] = BUCKET\n",
    "os.environ['REGION'] = REGION"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 3,
   "id": "beneficial-coaching",
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Tensorflow version: 2.3.2\n"
     ]
    }
   ],
   "source": [
    "# Import data processing libraries\n",
    "import pandas as pd\n",
    "import tensorflow as tf\n",
    "# Import Tensorflow Transform\n",
    "import tensorflow_transform as tft\n",
    "from tensorflow_transform.tf_metadata import dataset_metadata\n",
    "from tensorflow_transform.tf_metadata import dataset_schema\n",
    "# Import Apache Beam\n",
    "import apache_beam as beam\n",
    "from apache_beam.options.pipeline_options import PipelineOptions\n",
    "# Python shutil module enables us to operate with file objects easily and without diving into file objects a lot.\n",
    "import shutil\n",
    "# Show the currently installed version of TensorFlow\n",
    "print(\"Tensorflow version: {}\".format(tf.__version__))"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "conservative-judges",
   "metadata": {},
   "source": [
    "## Input source: CSV Local"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 4,
   "id": "french-theater",
   "metadata": {},
   "outputs": [],
   "source": [
    "# Import data locally only the fist 100 rows to support\n",
    "# the creation of the processing functions\n",
    "\n",
    "# TODO update path to bucket\n",
    "path_to_csv = os.path.join(LOCAL_ROOT_DIR, INPUT_FILE)\n",
    "data = pd.read_csv(path_to_csv, nrows=100)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 5,
   "id": "comfortable-candy",
   "metadata": {},
   "outputs": [
    {
     "data": {
      "text/plain": [
       "Index(['POSTED_BY', 'UNDER_CONSTRUCTION', 'RERA', 'BHK_NO.', 'BHK_OR_RK',\n",
       "       'SQUARE_FT', 'READY_TO_MOVE', 'RESALE', 'ADDRESS', 'LONGITUDE',\n",
       "       'LATITUDE', 'TARGET(PRICE_IN_LACS)'],\n",
       "      dtype='object')"
      ]
     },
     "execution_count": 5,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "data.columns"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 6,
   "id": "macro-album",
   "metadata": {},
   "outputs": [
    {
     "data": {
      "text/html": [
       "<div>\n",
       "<style scoped>\n",
       "    .dataframe tbody tr th:only-of-type {\n",
       "        vertical-align: middle;\n",
       "    }\n",
       "\n",
       "    .dataframe tbody tr th {\n",
       "        vertical-align: top;\n",
       "    }\n",
       "\n",
       "    .dataframe thead th {\n",
       "        text-align: right;\n",
       "    }\n",
       "</style>\n",
       "<table border=\"1\" class=\"dataframe\">\n",
       "  <thead>\n",
       "    <tr style=\"text-align: right;\">\n",
       "      <th></th>\n",
       "      <th>POSTED_BY</th>\n",
       "      <th>UNDER_CONSTRUCTION</th>\n",
       "      <th>RERA</th>\n",
       "      <th>BHK_NO.</th>\n",
       "      <th>BHK_OR_RK</th>\n",
       "      <th>SQUARE_FT</th>\n",
       "      <th>READY_TO_MOVE</th>\n",
       "      <th>RESALE</th>\n",
       "      <th>ADDRESS</th>\n",
       "      <th>LONGITUDE</th>\n",
       "      <th>LATITUDE</th>\n",
       "      <th>TARGET(PRICE_IN_LACS)</th>\n",
       "    </tr>\n",
       "  </thead>\n",
       "  <tbody>\n",
       "    <tr>\n",
       "      <th>0</th>\n",
       "      <td>Owner</td>\n",
       "      <td>0</td>\n",
       "      <td>0</td>\n",
       "      <td>2</td>\n",
       "      <td>BHK</td>\n",
       "      <td>1300.236407</td>\n",
       "      <td>1</td>\n",
       "      <td>1</td>\n",
       "      <td>Ksfc Layout,Bangalore</td>\n",
       "      <td>12.969910</td>\n",
       "      <td>77.597960</td>\n",
       "      <td>55.0</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>1</th>\n",
       "      <td>Dealer</td>\n",
       "      <td>0</td>\n",
       "      <td>0</td>\n",
       "      <td>2</td>\n",
       "      <td>BHK</td>\n",
       "      <td>1275.000000</td>\n",
       "      <td>1</td>\n",
       "      <td>1</td>\n",
       "      <td>Vishweshwara Nagar,Mysore</td>\n",
       "      <td>12.274538</td>\n",
       "      <td>76.644605</td>\n",
       "      <td>51.0</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>2</th>\n",
       "      <td>Owner</td>\n",
       "      <td>0</td>\n",
       "      <td>0</td>\n",
       "      <td>2</td>\n",
       "      <td>BHK</td>\n",
       "      <td>933.159722</td>\n",
       "      <td>1</td>\n",
       "      <td>1</td>\n",
       "      <td>Jigani,Bangalore</td>\n",
       "      <td>12.778033</td>\n",
       "      <td>77.632191</td>\n",
       "      <td>43.0</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>3</th>\n",
       "      <td>Owner</td>\n",
       "      <td>0</td>\n",
       "      <td>1</td>\n",
       "      <td>2</td>\n",
       "      <td>BHK</td>\n",
       "      <td>929.921143</td>\n",
       "      <td>1</td>\n",
       "      <td>1</td>\n",
       "      <td>Sector-1 Vaishali,Ghaziabad</td>\n",
       "      <td>28.642300</td>\n",
       "      <td>77.344500</td>\n",
       "      <td>62.5</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>4</th>\n",
       "      <td>Dealer</td>\n",
       "      <td>1</td>\n",
       "      <td>0</td>\n",
       "      <td>2</td>\n",
       "      <td>BHK</td>\n",
       "      <td>999.009247</td>\n",
       "      <td>0</td>\n",
       "      <td>1</td>\n",
       "      <td>New Town,Kolkata</td>\n",
       "      <td>22.592200</td>\n",
       "      <td>88.484911</td>\n",
       "      <td>60.5</td>\n",
       "    </tr>\n",
       "  </tbody>\n",
       "</table>\n",
       "</div>"
      ],
      "text/plain": [
       "  POSTED_BY  UNDER_CONSTRUCTION  RERA  BHK_NO. BHK_OR_RK    SQUARE_FT  \\\n",
       "0     Owner                   0     0        2       BHK  1300.236407   \n",
       "1    Dealer                   0     0        2       BHK  1275.000000   \n",
       "2     Owner                   0     0        2       BHK   933.159722   \n",
       "3     Owner                   0     1        2       BHK   929.921143   \n",
       "4    Dealer                   1     0        2       BHK   999.009247   \n",
       "\n",
       "   READY_TO_MOVE  RESALE                      ADDRESS  LONGITUDE   LATITUDE  \\\n",
       "0              1       1        Ksfc Layout,Bangalore  12.969910  77.597960   \n",
       "1              1       1    Vishweshwara Nagar,Mysore  12.274538  76.644605   \n",
       "2              1       1             Jigani,Bangalore  12.778033  77.632191   \n",
       "3              1       1  Sector-1 Vaishali,Ghaziabad  28.642300  77.344500   \n",
       "4              0       1             New Town,Kolkata  22.592200  88.484911   \n",
       "\n",
       "   TARGET(PRICE_IN_LACS)  \n",
       "0                   55.0  \n",
       "1                   51.0  \n",
       "2                   43.0  \n",
       "3                   62.5  \n",
       "4                   60.5  "
      ]
     },
     "execution_count": 6,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "data.head()"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 7,
   "id": "numeric-investing",
   "metadata": {},
   "outputs": [],
   "source": [
    "# Convert csv in a bigquery like processor\n",
    "data_bq_like = list(data.iloc[:5].to_dict(\"index\").values()) # sample just the first 10 rows to test the processing functions"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "polish-adult",
   "metadata": {},
   "source": [
    "## Create raw data metadata"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 22,
   "id": "flush-cigarette",
   "metadata": {},
   "outputs": [],
   "source": [
    "CATEGORICAL_FEATURE_NAMES_STRING = ['POSTED_BY', 'BHK_OR_RK', 'ADDRESS']\n",
    "#CATEGORICAL_FEATURE_MAX_VALUES = [24]\n",
    "CATEGORICAL_FEATURE_NAMES_INT = ['UNDER_CONSTRUCTION', 'RERA',\n",
    "                                 'READY_TO_MOVE', 'RESALE']\n",
    "\n",
    "NUMERIC_FEATURE_NAMES = ['BHK','SQUARE_FT', 'LONGITUDE','LATITUDE']\n",
    "#BUCKET_FEATURE_BUCKET_COUNT = [100]\n",
    "\n",
    "TARGET_FEATURE_NAME = \"TARGET_PRICE\""
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 23,
   "id": "excited-frank",
   "metadata": {},
   "outputs": [],
   "source": [
    "def create_raw_metadata():\n",
    "    \n",
    "    raw_data_schema = {}\n",
    "    \n",
    " \n",
    "    # target feature schema\n",
    "    raw_data_schema[TARGET_FEATURE_NAME]= dataset_schema.ColumnSchema(\n",
    "            tf.float32, [], dataset_schema.FixedColumnRepresentation())\n",
    "    \n",
    "    # categorical string features schema\n",
    "    raw_data_schema.update({column_name : dataset_schema.ColumnSchema(\n",
    "        tf.string, [], dataset_schema.FixedColumnRepresentation())\n",
    "                            for column_name in CATEGORICAL_FEATURE_NAMES_STRING})\n",
    "    \n",
    "    # categorical string features schema\n",
    "    raw_data_schema.update({column_name : dataset_schema.ColumnSchema(\n",
    "        tf.int64, [], dataset_schema.FixedColumnRepresentation())\n",
    "                            for column_name in CATEGORICAL_FEATURE_NAMES_INT})\n",
    "    \n",
    "    # numerical features schema\n",
    "    raw_data_schema.update({ column_name : dataset_schema.ColumnSchema(\n",
    "        tf.float32, [], dataset_schema.FixedColumnRepresentation())\n",
    "                            for column_name in NUMERIC_FEATURE_NAMES})\n",
    "    \n",
    "    # create dataset_metadata given raw_schema\n",
    "    raw_data_metadata = dataset_metadata.DatasetMetadata(\n",
    "        dataset_schema.Schema(raw_data_schema))\n",
    "    \n",
    "    return raw_data_metadata"
   ]
  },
  {
   "cell_type": "raw",
   "id": "hazardous-grill",
   "metadata": {},
   "source": [
    "# DEBUG validation of data_schema function\n",
    "create_raw_metadata()"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "registered-throw",
   "metadata": {},
   "source": [
    "## Input source: BigQuery"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 25,
   "id": "departmental-still",
   "metadata": {},
   "outputs": [],
   "source": [
    "from google.cloud import bigquery\n",
    "\n",
    "def create_query(phase, hash_column, table_name, EVERY_N=None):\n",
    "    \"\"\"Creates a query with the proper splits.\n",
    "    Args:\n",
    "        phase: str, train, valid.\n",
    "        hash_column: str, is the column that we will use to perform the split\n",
    "        EVERY_N: int, take an example EVERY_N rows.\n",
    "\n",
    "    Returns:\n",
    "        Query string with the proper splits.\n",
    "    \"\"\"\n",
    "    base_query = \"\"\"\n",
    "    SELECT POSTED_BY,\n",
    "    UNDER_CONSTRUCTION, \n",
    "    RERA, \n",
    "    BHK_NO_+0.0 AS BHK,\n",
    "    BHK_OR_RK,\n",
    "    SQUARE_FT,\n",
    "    READY_TO_MOVE,\n",
    "    RESALE,\n",
    "    ADDRESS,\n",
    "    LONGITUDE,\n",
    "    LATITUDE,\n",
    "    TARGET_PRICE_IN_LACS_ AS TARGET_PRICE,\n",
    "    FROM {0}\n",
    "    \"\"\".format(table_name)\n",
    "    \n",
    "    if EVERY_N is None:\n",
    "        if phase==\"train\":\n",
    "            # training\n",
    "            query = \"\"\"{0} WHERE MOD(ABS(FARM_FINGERPRINT(CAST({1} AS STRING))),\n",
    "            10) < 8\"\"\".format(base_query, hash_column)\n",
    "        else:\n",
    "            query = \"\"\"{0} WHERE MOD(ABS(FARM_FINGERPRINT(CAST({1} AS STRING))),\n",
    "            10) >= 8\"\"\".format(base_query, hash_column)\n",
    "    else:\n",
    "        query = \"\"\"{0} WHERE MOD(ABS(FARM_FINGERPRINT(CAST({1} AS STRING))),\n",
    "        {2})) = 2\"\"\".format(base_query,hash_column, EVERY_N)\n",
    "        # random split WHERE RAND() < 0.8\n",
    "\n",
    "    return query"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 26,
   "id": "imperial-arnold",
   "metadata": {},
   "outputs": [
    {
     "data": {
      "text/html": [
       "<div>\n",
       "<style scoped>\n",
       "    .dataframe tbody tr th:only-of-type {\n",
       "        vertical-align: middle;\n",
       "    }\n",
       "\n",
       "    .dataframe tbody tr th {\n",
       "        vertical-align: top;\n",
       "    }\n",
       "\n",
       "    .dataframe thead th {\n",
       "        text-align: right;\n",
       "    }\n",
       "</style>\n",
       "<table border=\"1\" class=\"dataframe\">\n",
       "  <thead>\n",
       "    <tr style=\"text-align: right;\">\n",
       "      <th></th>\n",
       "      <th>POSTED_BY</th>\n",
       "      <th>UNDER_CONSTRUCTION</th>\n",
       "      <th>RERA</th>\n",
       "      <th>BHK</th>\n",
       "      <th>BHK_OR_RK</th>\n",
       "      <th>SQUARE_FT</th>\n",
       "      <th>READY_TO_MOVE</th>\n",
       "      <th>RESALE</th>\n",
       "      <th>ADDRESS</th>\n",
       "      <th>LONGITUDE</th>\n",
       "      <th>LATITUDE</th>\n",
       "      <th>TARGET_PRICE</th>\n",
       "    </tr>\n",
       "  </thead>\n",
       "  <tbody>\n",
       "    <tr>\n",
       "      <th>0</th>\n",
       "      <td>Dealer</td>\n",
       "      <td>1</td>\n",
       "      <td>1</td>\n",
       "      <td>2.0</td>\n",
       "      <td>BHK</td>\n",
       "      <td>985.291154</td>\n",
       "      <td>0</td>\n",
       "      <td>1</td>\n",
       "      <td>Moshi,Pune</td>\n",
       "      <td>18.669919</td>\n",
       "      <td>73.853723</td>\n",
       "      <td>48.9</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>1</th>\n",
       "      <td>Dealer</td>\n",
       "      <td>1</td>\n",
       "      <td>1</td>\n",
       "      <td>2.0</td>\n",
       "      <td>BHK</td>\n",
       "      <td>715.059588</td>\n",
       "      <td>0</td>\n",
       "      <td>1</td>\n",
       "      <td>Moshi,Pune</td>\n",
       "      <td>18.669919</td>\n",
       "      <td>73.853723</td>\n",
       "      <td>33.0</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>2</th>\n",
       "      <td>Dealer</td>\n",
       "      <td>1</td>\n",
       "      <td>1</td>\n",
       "      <td>2.0</td>\n",
       "      <td>BHK</td>\n",
       "      <td>1057.195953</td>\n",
       "      <td>0</td>\n",
       "      <td>1</td>\n",
       "      <td>Moshi,Pune</td>\n",
       "      <td>18.669919</td>\n",
       "      <td>73.853723</td>\n",
       "      <td>51.2</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>3</th>\n",
       "      <td>Dealer</td>\n",
       "      <td>1</td>\n",
       "      <td>0</td>\n",
       "      <td>1.0</td>\n",
       "      <td>BHK</td>\n",
       "      <td>592.800528</td>\n",
       "      <td>0</td>\n",
       "      <td>1</td>\n",
       "      <td>Moshi,Pune</td>\n",
       "      <td>18.669919</td>\n",
       "      <td>73.853723</td>\n",
       "      <td>35.9</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>4</th>\n",
       "      <td>Dealer</td>\n",
       "      <td>1</td>\n",
       "      <td>1</td>\n",
       "      <td>2.0</td>\n",
       "      <td>BHK</td>\n",
       "      <td>988.175676</td>\n",
       "      <td>0</td>\n",
       "      <td>1</td>\n",
       "      <td>Moshi,Pune</td>\n",
       "      <td>18.669919</td>\n",
       "      <td>73.853723</td>\n",
       "      <td>46.8</td>\n",
       "    </tr>\n",
       "  </tbody>\n",
       "</table>\n",
       "</div>"
      ],
      "text/plain": [
       "  POSTED_BY  UNDER_CONSTRUCTION  RERA  BHK BHK_OR_RK    SQUARE_FT  \\\n",
       "0    Dealer                   1     1  2.0       BHK   985.291154   \n",
       "1    Dealer                   1     1  2.0       BHK   715.059588   \n",
       "2    Dealer                   1     1  2.0       BHK  1057.195953   \n",
       "3    Dealer                   1     0  1.0       BHK   592.800528   \n",
       "4    Dealer                   1     1  2.0       BHK   988.175676   \n",
       "\n",
       "   READY_TO_MOVE  RESALE     ADDRESS  LONGITUDE   LATITUDE  TARGET_PRICE  \n",
       "0              0       1  Moshi,Pune  18.669919  73.853723          48.9  \n",
       "1              0       1  Moshi,Pune  18.669919  73.853723          33.0  \n",
       "2              0       1  Moshi,Pune  18.669919  73.853723          51.2  \n",
       "3              0       1  Moshi,Pune  18.669919  73.853723          35.9  \n",
       "4              0       1  Moshi,Pune  18.669919  73.853723          46.8  "
      ]
     },
     "metadata": {},
     "output_type": "display_data"
    }
   ],
   "source": [
    "# DEBUG validation of the BigQuery function\n",
    "query = create_query(\"valid\", \"ADDRESS\", \"houseprice_data.train\")\n",
    "df_validation = bigquery.Client().query(query).to_dataframe()\n",
    "# `head()` function is used to get the first n rows of dataframe\n",
    "display(df_validation.head())"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 27,
   "id": "german-carnival",
   "metadata": {},
   "outputs": [],
   "source": [
    "def is_valid(inputs):\n",
    "    \"\"\"Check to make sure the inputs are valid.\n",
    "    Args:\n",
    "        inputs: dict, dictionary of TableRow data from BigQuery.\n",
    "\n",
    "    Returns:\n",
    "        True if the inputs are valid and False if they are not.\n",
    "    \"\"\"\n",
    "    try:\n",
    "        underconstruction = inputs[\"UNDER_CONSTRUCTION\"]\n",
    "        return underconstruction >= 0 and underconstruction  <2\n",
    "    except:\n",
    "        return False"
   ]
  },
  {
   "cell_type": "raw",
   "id": "noble-collar",
   "metadata": {},
   "source": [
    "# DEBUG validation of the is_valid function:\n",
    "for element in data_bq_like:\n",
    "    print(is_valid(element))"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 28,
   "id": "round-device",
   "metadata": {},
   "outputs": [],
   "source": [
    "def preprocess_tft(inputs):\n",
    "    \"\"\"\n",
    "    Preprocess the features and add engineered features with tf transform.\n",
    "\n",
    "    Args:\n",
    "        dict, dictionary of TableRow data from BigQuery.\n",
    "\n",
    "    Returns:\n",
    "        Dictionary of preprocessed data after scaling and feature engineering.\n",
    "    \"\"\"\n",
    "    # create a new dictionary that will contain only those relevan features selected\n",
    "    result = {}\n",
    "    # Report the label in the processed data set\n",
    "    result[\"target_price\"] = tf.identity(inputs[\"TARGET_PRICE\"])\n",
    "    \n",
    "    # Build a vocobulary\n",
    "    result[\"address\"] = tft.compute_and_apply_vocabulary(inputs[\"ADDRESS\"]) #remove the address and leave the city\n",
    "    result[\"posted_by\"] = tft.compute_and_apply_vocabulary(inputs[\"POSTED_BY\"])\n",
    "    result[\"bhk_or_rk\"] = tft.compute_and_apply_vocabulary(inputs[\"BHK_OR_RK\"])\n",
    "    \n",
    "    # Keep the  categorical int features untouch\n",
    "    result[\"under_construction\"] = tf.identity(inputs[\"UNDER_CONSTRUCTION\"])\n",
    "    result[\"rera\"] = tf.identity(inputs[\"RERA\"])\n",
    "    result[\"ready_to_move\"] = tf.identity(inputs[\"READY_TO_MOVE\"])\n",
    "    result[\"resale\"] = tf.identity(inputs[\"RESALE\"])\n",
    "\n",
    "    # Scaling numeric values\n",
    "    result['bhk_no'] = (tft.scale_to_0_1(inputs['BHK']))\n",
    "    result['square_ft'] = (tft.scale_to_0_1(inputs['SQUARE_FT']))\n",
    "\n",
    "    # Bucketize\n",
    "    result['longitude_bucket'] = tft.bucketize(inputs[\"LONGITUDE\"], num_buckets=100)\n",
    "    result['latitude_bucket'] = tft.bucketize(inputs[\"LATITUDE\"], num_buckets=100)\n",
    "    \n",
    "    \n",
    "    # Engineered features\n",
    "    # basic implementation of a feature created to (potentially) improve the prediction\n",
    "    result[\"avg_room_sq.ft\"] = tft.scale_to_0_1(inputs['SQUARE_FT'] / inputs['BHK'])\n",
    "    \n",
    "    return result"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "described-peoples",
   "metadata": {},
   "source": [
    "## Process function Apache Beam"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 29,
   "id": "capital-tamil",
   "metadata": {},
   "outputs": [],
   "source": [
    "def preprocess(in_test_mode):\n",
    "    \"\"\"Sets up preprocess pipeline.\n",
    "\n",
    "    Args:\n",
    "        in_test_mode: bool, False to launch DataFlow job, True to run locally.\n",
    "    \"\"\"\n",
    "    import os\n",
    "    import os.path\n",
    "    import tempfile\n",
    "    import datetime\n",
    "    from apache_beam.io import tfrecordio\n",
    "    from tensorflow_transform.coders import example_proto_coder\n",
    "    from tensorflow_transform.tf_metadata import dataset_metadata\n",
    "    from tensorflow_transform.tf_metadata import dataset_schema\n",
    "    from tensorflow_transform.beam import tft_beam_io\n",
    "    from tensorflow_transform.beam import impl as beam_impl\n",
    "    from tensorflow_transform.beam.tft_beam_io import transform_fn_io\n",
    "\n",
    "    job_name = 'house_price_tftprocessing' + '-'\n",
    "    job_name += datetime.datetime.now().strftime('%y%m%d-%H%M%S')\n",
    "    if in_test_mode:\n",
    "        import shutil\n",
    "        print('Launching local job ...')\n",
    "        #OUTPUT_DIR = './processing_tft' #local saving\n",
    "        OUTPUT_DIR = 'gs://{0}/processing_tft/'.format(BUCKET) # bucket saving\n",
    "        shutil.rmtree(OUTPUT_DIR, ignore_errors=True)\n",
    "        EVERY_N = None\n",
    "    else:\n",
    "        print('Launching Dataflow job {} ...'.format(job_name))\n",
    "        OUTPUT_DIR = 'gs://{0}/processing_tft/'.format(BUCKET)\n",
    "        import subprocess\n",
    "        subprocess.call('gsutil rm -r {}'.format(OUTPUT_DIR).split())\n",
    "        EVERY_N = None\n",
    "\n",
    "    options = {\n",
    "        'staging_location': os.path.join(OUTPUT_DIR, 'tmp', 'staging'),\n",
    "        'temp_location': os.path.join(OUTPUT_DIR, 'tmp'),\n",
    "        'job_name': job_name,\n",
    "        'project': PROJECT,\n",
    "        'num_workers': 1,\n",
    "        'max_num_workers': 1,\n",
    "        'teardown_policy': 'TEARDOWN_ALWAYS',\n",
    "        'no_save_main_session': True,\n",
    "        'direct_num_workers': 1,\n",
    "        'extra_packages': ['tensorflow_transform-0.24.0-py3-none-any.whl']\n",
    "        }\n",
    "\n",
    "    opts = beam.pipeline.PipelineOptions(flags=[], **options)\n",
    "    if in_test_mode:\n",
    "        RUNNER = 'DirectRunner'\n",
    "    else:\n",
    "        RUNNER = 'DataflowRunner'\n",
    "\n",
    "    # Set up raw data metadata\n",
    "    raw_data_metadata = create_raw_metadata()\n",
    "\n",
    "    # Run Beam\n",
    "    with beam.Pipeline(RUNNER, options=opts) as p:\n",
    "        with beam_impl.Context(temp_dir=os.path.join(OUTPUT_DIR, 'tmp')):\n",
    "            # Save the raw data metadata\n",
    "            (raw_data_metadata |\n",
    "                'WriteInputMetadata' >> tft_beam_io.WriteMetadata(\n",
    "                    os.path.join(\n",
    "                        OUTPUT_DIR, 'metadata/rawdata_metadata'), pipeline=p))\n",
    "\n",
    "            # Read training data from bigquery and filter rows\n",
    "            raw_data = (p \n",
    "                        | 'Read train set' >> beam.io.Read(beam.io.BigQuerySource(query=create_query(\"train\", \"ADDRESS\", \"houseprice_data.train\"),\n",
    "                                                                              use_standard_sql=True)) \n",
    "                        | 'Validate data' >> beam.Filter(is_valid))\n",
    "\n",
    "            raw_dataset = (raw_data, raw_data_metadata)\n",
    "\n",
    "            # Analyze and transform training data\n",
    "            transformed_dataset, transform_fn = (raw_dataset \n",
    "                                                 | beam_impl.AnalyzeAndTransformDataset(preprocess_tft))\n",
    "            \n",
    "            transformed_data, transformed_metadata = transformed_dataset\n",
    "\n",
    "            # Save transformed train data to disk in efficient tfrecord format\n",
    "            transformed_data | 'WriteTrainData' >> tfrecordio.WriteToTFRecord(os.path.join(OUTPUT_DIR, 'train'),\n",
    "                                                                              file_name_suffix='.gz',\n",
    "                                                                              coder=example_proto_coder.ExampleProtoCoder(transformed_metadata.schema))\n",
    "\n",
    "            \n",
    "            # Read eval data from bigquery and filter rows\n",
    "            raw_test_data = (p \n",
    "                             | 'Read eval set' >> beam.io.Read(beam.io.BigQuerySource(query=create_query(\"valid\", \"ADDRESS\", \"houseprice_data.train\"),\n",
    "                                                                                        use_standard_sql=True)) \n",
    "                             | 'eval_filter' >> beam.Filter(is_valid))\n",
    "\n",
    "            raw_test_dataset = (raw_test_data, raw_data_metadata)\n",
    "\n",
    "            # Transform eval data\n",
    "            transformed_test_dataset = ((raw_test_dataset, transform_fn) | beam_impl.TransformDataset())\n",
    "            transformed_test_data, _ = transformed_test_dataset\n",
    "\n",
    "            # Save transformed train data to disk in efficient tfrecord format\n",
    "            (transformed_test_data | 'WriteTestData' >> tfrecordio.WriteToTFRecord(os.path.join(OUTPUT_DIR, 'eval'), \n",
    "                                                                                   file_name_suffix='.gz',\n",
    "                                                                                   coder=example_proto_coder.ExampleProtoCoder(transformed_metadata.schema)))\n",
    "\n",
    "            # Save transformation function to disk for use at serving time\n",
    "            (transform_fn | 'WriteTransformFn' >> transform_fn_io.WriteTransformFn(os.path.join(OUTPUT_DIR, 'metadata')))"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 30,
   "id": "creative-butterfly",
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Launching local job ...\n",
      "WARNING:tensorflow:Tensorflow version (2.3.2) found. Note that Tensorflow Transform support for TF 2.0 is currently in beta, and features such as tf.function may not work as intended. \n"
     ]
    },
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      "/opt/conda/lib/python3.7/site-packages/ipykernel_launcher.py:70: BeamDeprecationWarning: BigQuerySource is deprecated since 2.25.0. Use ReadFromBigQuery instead.\n",
      "/opt/conda/lib/python3.7/site-packages/apache_beam/io/gcp/bigquery.py:1881: BeamDeprecationWarning: options is deprecated since First stable release. References to <pipeline>.options will not be supported\n",
      "  temp_location = pcoll.pipeline.options.view_as(\n",
      "WARNING:tensorflow:Tensorflow version (2.3.2) found. Note that Tensorflow Transform support for TF 2.0 is currently in beta, and features such as tf.function may not work as intended. \n"
     ]
    },
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "WARNING:tensorflow:Tensorflow version (2.3.2) found. Note that Tensorflow Transform support for TF 2.0 is currently in beta, and features such as tf.function may not work as intended. \n"
     ]
    },
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      "WARNING:tensorflow:Tensorflow version (2.3.2) found. Note that Tensorflow Transform support for TF 2.0 is currently in beta, and features such as tf.function may not work as intended. \n"
     ]
    },
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "WARNING:tensorflow:You are passing instance dicts and DatasetMetadata to TFT which will not provide optimal performance. Consider following the TFT guide to upgrade to the TFXIO format (Apache Arrow RecordBatch).\n"
     ]
    },
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      "WARNING:tensorflow:You are passing instance dicts and DatasetMetadata to TFT which will not provide optimal performance. Consider following the TFT guide to upgrade to the TFXIO format (Apache Arrow RecordBatch).\n"
     ]
    },
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "WARNING:tensorflow:From /opt/conda/lib/python3.7/site-packages/tensorflow_transform/tf_utils.py:261: Tensor.experimental_ref (from tensorflow.python.framework.ops) is deprecated and will be removed in a future version.\n",
      "Instructions for updating:\n",
      "Use ref() instead.\n"
     ]
    },
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      "WARNING:tensorflow:From /opt/conda/lib/python3.7/site-packages/tensorflow_transform/tf_utils.py:261: Tensor.experimental_ref (from tensorflow.python.framework.ops) is deprecated and will be removed in a future version.\n",
      "Instructions for updating:\n",
      "Use ref() instead.\n"
     ]
    },
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "WARNING:tensorflow:From /opt/conda/lib/python3.7/site-packages/tensorflow/python/saved_model/signature_def_utils_impl.py:201: build_tensor_info (from tensorflow.python.saved_model.utils_impl) is deprecated and will be removed in a future version.\n",
      "Instructions for updating:\n",
      "This function will only be available through the v1 compatibility library as tf.compat.v1.saved_model.utils.build_tensor_info or tf.compat.v1.saved_model.build_tensor_info.\n"
     ]
    },
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      "WARNING:tensorflow:From /opt/conda/lib/python3.7/site-packages/tensorflow/python/saved_model/signature_def_utils_impl.py:201: build_tensor_info (from tensorflow.python.saved_model.utils_impl) is deprecated and will be removed in a future version.\n",
      "Instructions for updating:\n",
      "This function will only be available through the v1 compatibility library as tf.compat.v1.saved_model.utils.build_tensor_info or tf.compat.v1.saved_model.build_tensor_info.\n"
     ]
    },
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "INFO:tensorflow:Assets added to graph.\n"
     ]
    },
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      "INFO:tensorflow:Assets added to graph.\n"
     ]
    },
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "INFO:tensorflow:No assets to write.\n"
     ]
    },
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      "INFO:tensorflow:No assets to write.\n"
     ]
    },
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "WARNING:tensorflow:Issue encountered when serializing tft_mapper_use.\n",
      "Type is unsupported, or the types of the items don't match field type in CollectionDef. Note this is a warning and probably safe to ignore.\n",
      "'Counter' object has no attribute 'name'\n"
     ]
    },
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      "WARNING:tensorflow:Issue encountered when serializing tft_mapper_use.\n",
      "Type is unsupported, or the types of the items don't match field type in CollectionDef. Note this is a warning and probably safe to ignore.\n",
      "'Counter' object has no attribute 'name'\n"
     ]
    },
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "INFO:tensorflow:SavedModel written to: gs://houseprice_tftransform/processing_tft/tmp/tftransform_tmp/604c4d4bd5974699a85228c96bfa7251/saved_model.pb\n"
     ]
    },
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      "INFO:tensorflow:SavedModel written to: gs://houseprice_tftransform/processing_tft/tmp/tftransform_tmp/604c4d4bd5974699a85228c96bfa7251/saved_model.pb\n"
     ]
    },
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "INFO:tensorflow:Assets added to graph.\n"
     ]
    },
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      "INFO:tensorflow:Assets added to graph.\n"
     ]
    },
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "INFO:tensorflow:No assets to write.\n"
     ]
    },
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      "INFO:tensorflow:No assets to write.\n"
     ]
    },
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "WARNING:tensorflow:Issue encountered when serializing tft_mapper_use.\n",
      "Type is unsupported, or the types of the items don't match field type in CollectionDef. Note this is a warning and probably safe to ignore.\n",
      "'Counter' object has no attribute 'name'\n"
     ]
    },
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      "WARNING:tensorflow:Issue encountered when serializing tft_mapper_use.\n",
      "Type is unsupported, or the types of the items don't match field type in CollectionDef. Note this is a warning and probably safe to ignore.\n",
      "'Counter' object has no attribute 'name'\n"
     ]
    },
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "INFO:tensorflow:SavedModel written to: gs://houseprice_tftransform/processing_tft/tmp/tftransform_tmp/9bb0c04b9bc04a2286fffae2bd58b5e1/saved_model.pb\n"
     ]
    },
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      "INFO:tensorflow:SavedModel written to: gs://houseprice_tftransform/processing_tft/tmp/tftransform_tmp/9bb0c04b9bc04a2286fffae2bd58b5e1/saved_model.pb\n"
     ]
    },
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "WARNING:tensorflow:Tensorflow version (2.3.2) found. Note that Tensorflow Transform support for TF 2.0 is currently in beta, and features such as tf.function may not work as intended. \n"
     ]
    },
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      "WARNING:tensorflow:Tensorflow version (2.3.2) found. Note that Tensorflow Transform support for TF 2.0 is currently in beta, and features such as tf.function may not work as intended. \n"
     ]
    },
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "WARNING:tensorflow:You are passing instance dicts and DatasetMetadata to TFT which will not provide optimal performance. Consider following the TFT guide to upgrade to the TFXIO format (Apache Arrow RecordBatch).\n"
     ]
    },
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      "WARNING:tensorflow:You are passing instance dicts and DatasetMetadata to TFT which will not provide optimal performance. Consider following the TFT guide to upgrade to the TFXIO format (Apache Arrow RecordBatch).\n"
     ]
    },
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "WARNING:tensorflow:Tensorflow version (2.3.2) found. Note that Tensorflow Transform support for TF 2.0 is currently in beta, and features such as tf.function may not work as intended. \n"
     ]
    },
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      "WARNING:tensorflow:Tensorflow version (2.3.2) found. Note that Tensorflow Transform support for TF 2.0 is currently in beta, and features such as tf.function may not work as intended. \n"
     ]
    },
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "WARNING:tensorflow:You are passing instance dicts and DatasetMetadata to TFT which will not provide optimal performance. Consider following the TFT guide to upgrade to the TFXIO format (Apache Arrow RecordBatch).\n"
     ]
    },
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      "WARNING:tensorflow:You are passing instance dicts and DatasetMetadata to TFT which will not provide optimal performance. Consider following the TFT guide to upgrade to the TFXIO format (Apache Arrow RecordBatch).\n",
      "WARNING:apache_beam.io.gcp.bigquery_tools:Dataset ds-dev-playground:temp_dataset_bb51c0ea51694fedbb82e887b724aa6c does not exist so we will create it as temporary with location=US\n",
      "WARNING:apache_beam.io.gcp.bigquery_tools:Dataset ds-dev-playground:temp_dataset_2e9cd6b71b794c8cae7b2be7226ac08a does not exist so we will create it as temporary with location=US\n"
     ]
    },
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "INFO:tensorflow:Saver not created because there are no variables in the graph to restore\n"
     ]
    },
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      "INFO:tensorflow:Saver not created because there are no variables in the graph to restore\n"
     ]
    },
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "INFO:tensorflow:Saver not created because there are no variables in the graph to restore\n"
     ]
    },
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      "INFO:tensorflow:Saver not created because there are no variables in the graph to restore\n"
     ]
    },
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "INFO:tensorflow:Assets added to graph.\n"
     ]
    },
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      "INFO:tensorflow:Assets added to graph.\n"
     ]
    },
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "INFO:tensorflow:Assets written to: gs://houseprice_tftransform/processing_tft/tmp/tftransform_tmp/e0313453fde14198a3c521b13f24b9a3/assets\n"
     ]
    },
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      "INFO:tensorflow:Assets written to: gs://houseprice_tftransform/processing_tft/tmp/tftransform_tmp/e0313453fde14198a3c521b13f24b9a3/assets\n"
     ]
    },
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "INFO:tensorflow:SavedModel written to: gs://houseprice_tftransform/processing_tft/tmp/tftransform_tmp/e0313453fde14198a3c521b13f24b9a3/saved_model.pb\n"
     ]
    },
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      "INFO:tensorflow:SavedModel written to: gs://houseprice_tftransform/processing_tft/tmp/tftransform_tmp/e0313453fde14198a3c521b13f24b9a3/saved_model.pb\n"
     ]
    },
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "WARNING:tensorflow:Expected binary or unicode string, got type_url: \"type.googleapis.com/tensorflow.AssetFileDef\"\n",
      "value: \"\\n\\013\\n\\tConst_3:0\\022-vocab_compute_and_apply_vocabulary_vocabulary\"\n",
      "\n"
     ]
    },
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      "WARNING:tensorflow:Expected binary or unicode string, got type_url: \"type.googleapis.com/tensorflow.AssetFileDef\"\n",
      "value: \"\\n\\013\\n\\tConst_3:0\\022-vocab_compute_and_apply_vocabulary_vocabulary\"\n",
      "\n"
     ]
    },
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "WARNING:tensorflow:Expected binary or unicode string, got type_url: \"type.googleapis.com/tensorflow.AssetFileDef\"\n",
      "value: \"\\n\\013\\n\\tConst_5:0\\022/vocab_compute_and_apply_vocabulary_1_vocabulary\"\n",
      "\n"
     ]
    },
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      "WARNING:tensorflow:Expected binary or unicode string, got type_url: \"type.googleapis.com/tensorflow.AssetFileDef\"\n",
      "value: \"\\n\\013\\n\\tConst_5:0\\022/vocab_compute_and_apply_vocabulary_1_vocabulary\"\n",
      "\n"
     ]
    },
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "WARNING:tensorflow:Expected binary or unicode string, got type_url: \"type.googleapis.com/tensorflow.AssetFileDef\"\n",
      "value: \"\\n\\013\\n\\tConst_7:0\\022/vocab_compute_and_apply_vocabulary_2_vocabulary\"\n",
      "\n"
     ]
    },
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      "WARNING:tensorflow:Expected binary or unicode string, got type_url: \"type.googleapis.com/tensorflow.AssetFileDef\"\n",
      "value: \"\\n\\013\\n\\tConst_7:0\\022/vocab_compute_and_apply_vocabulary_2_vocabulary\"\n",
      "\n"
     ]
    },
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "INFO:tensorflow:Saver not created because there are no variables in the graph to restore\n"
     ]
    },
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      "INFO:tensorflow:Saver not created because there are no variables in the graph to restore\n"
     ]
    },
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "WARNING:tensorflow:Expected binary or unicode string, got type_url: \"type.googleapis.com/tensorflow.AssetFileDef\"\n",
      "value: \"\\n\\013\\n\\tConst_3:0\\022-vocab_compute_and_apply_vocabulary_vocabulary\"\n",
      "\n"
     ]
    },
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      "WARNING:tensorflow:Expected binary or unicode string, got type_url: \"type.googleapis.com/tensorflow.AssetFileDef\"\n",
      "value: \"\\n\\013\\n\\tConst_3:0\\022-vocab_compute_and_apply_vocabulary_vocabulary\"\n",
      "\n"
     ]
    },
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "WARNING:tensorflow:Expected binary or unicode string, got type_url: \"type.googleapis.com/tensorflow.AssetFileDef\"\n",
      "value: \"\\n\\013\\n\\tConst_5:0\\022/vocab_compute_and_apply_vocabulary_1_vocabulary\"\n",
      "\n"
     ]
    },
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      "WARNING:tensorflow:Expected binary or unicode string, got type_url: \"type.googleapis.com/tensorflow.AssetFileDef\"\n",
      "value: \"\\n\\013\\n\\tConst_5:0\\022/vocab_compute_and_apply_vocabulary_1_vocabulary\"\n",
      "\n"
     ]
    },
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "WARNING:tensorflow:Expected binary or unicode string, got type_url: \"type.googleapis.com/tensorflow.AssetFileDef\"\n",
      "value: \"\\n\\013\\n\\tConst_7:0\\022/vocab_compute_and_apply_vocabulary_2_vocabulary\"\n",
      "\n"
     ]
    },
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      "WARNING:tensorflow:Expected binary or unicode string, got type_url: \"type.googleapis.com/tensorflow.AssetFileDef\"\n",
      "value: \"\\n\\013\\n\\tConst_7:0\\022/vocab_compute_and_apply_vocabulary_2_vocabulary\"\n",
      "\n"
     ]
    },
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "INFO:tensorflow:Saver not created because there are no variables in the graph to restore\n"
     ]
    },
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      "INFO:tensorflow:Saver not created because there are no variables in the graph to restore\n",
      "WARNING:apache_beam.io.tfrecordio:Couldn't find python-snappy so the implementation of _TFRecordUtil._masked_crc32c is not as fast as it could be.\n"
     ]
    },
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "WARNING:tensorflow:Expected binary or unicode string, got type_url: \"type.googleapis.com/tensorflow.AssetFileDef\"\n",
      "value: \"\\n\\013\\n\\tConst_3:0\\022-vocab_compute_and_apply_vocabulary_vocabulary\"\n",
      "\n"
     ]
    },
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      "WARNING:tensorflow:Expected binary or unicode string, got type_url: \"type.googleapis.com/tensorflow.AssetFileDef\"\n",
      "value: \"\\n\\013\\n\\tConst_3:0\\022-vocab_compute_and_apply_vocabulary_vocabulary\"\n",
      "\n"
     ]
    },
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "WARNING:tensorflow:Expected binary or unicode string, got type_url: \"type.googleapis.com/tensorflow.AssetFileDef\"\n",
      "value: \"\\n\\013\\n\\tConst_5:0\\022/vocab_compute_and_apply_vocabulary_1_vocabulary\"\n",
      "\n"
     ]
    },
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      "WARNING:tensorflow:Expected binary or unicode string, got type_url: \"type.googleapis.com/tensorflow.AssetFileDef\"\n",
      "value: \"\\n\\013\\n\\tConst_5:0\\022/vocab_compute_and_apply_vocabulary_1_vocabulary\"\n",
      "\n"
     ]
    },
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "WARNING:tensorflow:Expected binary or unicode string, got type_url: \"type.googleapis.com/tensorflow.AssetFileDef\"\n",
      "value: \"\\n\\013\\n\\tConst_7:0\\022/vocab_compute_and_apply_vocabulary_2_vocabulary\"\n",
      "\n"
     ]
    },
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      "WARNING:tensorflow:Expected binary or unicode string, got type_url: \"type.googleapis.com/tensorflow.AssetFileDef\"\n",
      "value: \"\\n\\013\\n\\tConst_7:0\\022/vocab_compute_and_apply_vocabulary_2_vocabulary\"\n",
      "\n"
     ]
    },
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "INFO:tensorflow:Saver not created because there are no variables in the graph to restore\n"
     ]
    },
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      "INFO:tensorflow:Saver not created because there are no variables in the graph to restore\n"
     ]
    }
   ],
   "source": [
    "# Change to False to run on DataFlow\n",
    "preprocess(in_test_mode=True)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "cognitive-bolivia",
   "metadata": {},
   "outputs": [],
   "source": []
  }
 ],
 "metadata": {
  "environment": {
   "name": "tf2-gpu.2-3.mnightly-2021-01-20-debian-10-test",
   "type": "gcloud",
   "uri": "gcr.io/deeplearning-platform-release/tf2-gpu.2-3:mnightly-2021-01-20-debian-10-test"
  },
  "kernelspec": {
   "display_name": "Python 3",
   "language": "python",
   "name": "python3"
  },
  "language_info": {
   "codemirror_mode": {
    "name": "ipython",
    "version": 3
   },
   "file_extension": ".py",
   "mimetype": "text/x-python",
   "name": "python",
   "nbconvert_exporter": "python",
   "pygments_lexer": "ipython3",
   "version": "3.7.9"
  }
 },
 "nbformat": 4,
 "nbformat_minor": 5
}
